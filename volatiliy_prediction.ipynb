{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b92966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ee679fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca497f5",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86c03369",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('daily_rv_daily_data2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237aa88d",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa71a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_Y(df, window_size=21):\n",
    "  df_as_np = df.to_numpy()\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size):\n",
    "    row = [r for r in df_as_np[i:i+window_size]]\n",
    "    X.append(row)\n",
    "    label = [df_as_np[i+window_size]]\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd95f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df_to_X_Y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30cee356",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_80 = int(len(data.index) * .8)\n",
    "q_90 = int(len(data.index) * .9)\n",
    "\n",
    "X_train, Y_train =  X[:q_80], Y[:q_80]\n",
    "\n",
    "X_val, Y_val =  X[q_80:q_90], Y[q_80:q_90]\n",
    "X_test, Y_test =  X[q_90:], Y[q_90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96fe4a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 21, 100)           9800      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 21, 100)           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 21, 100)           10100     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 21, 100)           0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 21, 97)            9797      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29697 (116.00 KB)\n",
      "Trainable params: 29697 (116.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from keras import activations\n",
    "\n",
    "MLP = Sequential([layers.Input((21, 97)),\n",
    "                    layers.Dense(100),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(100),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(97, activation=activations.relu)])\n",
    "\n",
    "MLP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44b6fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.compile(loss='mse', \n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30187e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0530 - val_loss: 0.0216 - val_mean_absolute_error: 0.1128\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0268 - val_mean_absolute_error: 0.1302\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0212 - val_mean_absolute_error: 0.1111\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0542 - val_loss: 0.0212 - val_mean_absolute_error: 0.1113\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0194 - val_mean_absolute_error: 0.1054\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0536 - val_loss: 0.0243 - val_mean_absolute_error: 0.1215\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0212 - val_mean_absolute_error: 0.1114\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0207 - val_mean_absolute_error: 0.1100\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0213 - val_mean_absolute_error: 0.1112\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0224 - val_mean_absolute_error: 0.1154\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0217 - val_mean_absolute_error: 0.1128\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0235 - val_mean_absolute_error: 0.1197\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0067 - mean_absolute_error: 0.0550 - val_loss: 0.0229 - val_mean_absolute_error: 0.1157\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0536 - val_loss: 0.0203 - val_mean_absolute_error: 0.1086\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0201 - val_mean_absolute_error: 0.1073\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0188 - val_mean_absolute_error: 0.1035\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0541 - val_loss: 0.0209 - val_mean_absolute_error: 0.1101\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0247 - val_mean_absolute_error: 0.1221\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0199 - val_mean_absolute_error: 0.1071\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0280 - val_mean_absolute_error: 0.1325\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0241 - val_mean_absolute_error: 0.1207\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0531 - val_loss: 0.0221 - val_mean_absolute_error: 0.1139\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0200 - val_mean_absolute_error: 0.1067\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0541 - val_loss: 0.0199 - val_mean_absolute_error: 0.1067\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0202 - val_mean_absolute_error: 0.1078\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0266 - val_mean_absolute_error: 0.1277\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0220 - val_mean_absolute_error: 0.1149\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0218 - val_mean_absolute_error: 0.1137\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0177 - val_mean_absolute_error: 0.0998\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0066 - mean_absolute_error: 0.0552 - val_loss: 0.0203 - val_mean_absolute_error: 0.1084\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0065 - mean_absolute_error: 0.0544 - val_loss: 0.0206 - val_mean_absolute_error: 0.1095\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0213 - val_mean_absolute_error: 0.1119\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0532 - val_loss: 0.0195 - val_mean_absolute_error: 0.1058\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0261 - val_mean_absolute_error: 0.1263\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0240 - val_mean_absolute_error: 0.1200\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0232 - val_mean_absolute_error: 0.1182\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0542 - val_loss: 0.0236 - val_mean_absolute_error: 0.1185\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0540 - val_loss: 0.0234 - val_mean_absolute_error: 0.1190\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0061 - mean_absolute_error: 0.0527 - val_loss: 0.0209 - val_mean_absolute_error: 0.1111\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0214 - val_mean_absolute_error: 0.1121\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0542 - val_loss: 0.0156 - val_mean_absolute_error: 0.0923\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0543 - val_loss: 0.0219 - val_mean_absolute_error: 0.1143\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0190 - val_mean_absolute_error: 0.1041\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0546 - val_loss: 0.0230 - val_mean_absolute_error: 0.1176\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0534 - val_loss: 0.0208 - val_mean_absolute_error: 0.1097\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0265 - val_mean_absolute_error: 0.1279\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0530 - val_loss: 0.0243 - val_mean_absolute_error: 0.1207\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0536 - val_loss: 0.0239 - val_mean_absolute_error: 0.1197\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0205 - val_mean_absolute_error: 0.1089\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0534 - val_loss: 0.0221 - val_mean_absolute_error: 0.1144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0061 - mean_absolute_error: 0.0532 - val_loss: 0.0256 - val_mean_absolute_error: 0.1258\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0539 - val_loss: 0.0291 - val_mean_absolute_error: 0.1358\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0540 - val_loss: 0.0219 - val_mean_absolute_error: 0.1140\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0537 - val_loss: 0.0215 - val_mean_absolute_error: 0.1114\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0540 - val_loss: 0.0239 - val_mean_absolute_error: 0.1194\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0219 - val_mean_absolute_error: 0.1135\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0243 - val_mean_absolute_error: 0.1215\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0240 - val_mean_absolute_error: 0.1203\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0528 - val_loss: 0.0219 - val_mean_absolute_error: 0.1132\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0240 - val_mean_absolute_error: 0.1203\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0304 - val_mean_absolute_error: 0.1402\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0066 - mean_absolute_error: 0.0551 - val_loss: 0.0236 - val_mean_absolute_error: 0.1183\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0064 - mean_absolute_error: 0.0540 - val_loss: 0.0200 - val_mean_absolute_error: 0.1073\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0233 - val_mean_absolute_error: 0.1177\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0064 - mean_absolute_error: 0.0540 - val_loss: 0.0235 - val_mean_absolute_error: 0.1185\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0065 - mean_absolute_error: 0.0550 - val_loss: 0.0217 - val_mean_absolute_error: 0.1125\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0065 - mean_absolute_error: 0.0542 - val_loss: 0.0192 - val_mean_absolute_error: 0.1042\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0066 - mean_absolute_error: 0.0553 - val_loss: 0.0195 - val_mean_absolute_error: 0.1059\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0065 - mean_absolute_error: 0.0550 - val_loss: 0.0227 - val_mean_absolute_error: 0.1160\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0532 - val_loss: 0.0192 - val_mean_absolute_error: 0.1045\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0254 - val_mean_absolute_error: 0.1246\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0232 - val_mean_absolute_error: 0.1180\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0287 - val_mean_absolute_error: 0.1348\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0210 - val_mean_absolute_error: 0.1105\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 1s 21ms/step - loss: 0.0064 - mean_absolute_error: 0.0538 - val_loss: 0.0223 - val_mean_absolute_error: 0.1148\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0233 - val_mean_absolute_error: 0.1180\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0227 - val_mean_absolute_error: 0.1159\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.0061 - mean_absolute_error: 0.0531 - val_loss: 0.0263 - val_mean_absolute_error: 0.1266\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0258 - val_mean_absolute_error: 0.1260\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0062 - mean_absolute_error: 0.0536 - val_loss: 0.0268 - val_mean_absolute_error: 0.1289\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 0.0065 - mean_absolute_error: 0.0546 - val_loss: 0.0250 - val_mean_absolute_error: 0.1230\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0238 - val_mean_absolute_error: 0.1197\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0170 - val_mean_absolute_error: 0.0971\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 1s 20ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0263 - val_mean_absolute_error: 0.1269\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0236 - val_mean_absolute_error: 0.1190\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0221 - val_mean_absolute_error: 0.1142\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0065 - mean_absolute_error: 0.0542 - val_loss: 0.0170 - val_mean_absolute_error: 0.0973\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0538 - val_loss: 0.0193 - val_mean_absolute_error: 0.1045\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0063 - mean_absolute_error: 0.0534 - val_loss: 0.0175 - val_mean_absolute_error: 0.0988\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0061 - mean_absolute_error: 0.0531 - val_loss: 0.0195 - val_mean_absolute_error: 0.1051\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0252 - val_mean_absolute_error: 0.1234\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0255 - val_mean_absolute_error: 0.1247\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0066 - mean_absolute_error: 0.0553 - val_loss: 0.0262 - val_mean_absolute_error: 0.1262\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0188 - val_mean_absolute_error: 0.1030\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0206 - val_mean_absolute_error: 0.1092\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0542 - val_loss: 0.0236 - val_mean_absolute_error: 0.1182\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0218 - val_mean_absolute_error: 0.1134\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0061 - mean_absolute_error: 0.0530 - val_loss: 0.0234 - val_mean_absolute_error: 0.1186\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0530 - val_loss: 0.0220 - val_mean_absolute_error: 0.1136\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0062 - mean_absolute_error: 0.0537 - val_loss: 0.0224 - val_mean_absolute_error: 0.1152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27b022b2b10>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e8d857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "MLP_pred = MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27999431",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f75887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_MLP = mse(Y_test, MLP_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ea604",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "172199a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 21, 100)           79200     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 21, 100)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 21, 100)           80400     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 21, 100)           0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 21, 97)            9797      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169397 (661.71 KB)\n",
      "Trainable params: 169397 (661.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM = Sequential([layers.Input((21, 97)),\n",
    "                    layers.LSTM(100, return_sequences=True),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.LSTM(100, return_sequences=True),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(97, activation=activations.relu)])\n",
    "\n",
    "LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd59bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM.compile(loss='mse', \n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b8eac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70/70 [==============================] - 12s 78ms/step - loss: 0.0256 - mean_absolute_error: 0.1089 - val_loss: 0.0136 - val_mean_absolute_error: 0.0820\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0121 - mean_absolute_error: 0.0733 - val_loss: 0.0129 - val_mean_absolute_error: 0.0784\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0104 - mean_absolute_error: 0.0672 - val_loss: 0.0115 - val_mean_absolute_error: 0.0790\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0098 - mean_absolute_error: 0.0666 - val_loss: 0.0124 - val_mean_absolute_error: 0.0775\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0089 - mean_absolute_error: 0.0630 - val_loss: 0.0124 - val_mean_absolute_error: 0.0774\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0087 - mean_absolute_error: 0.0626 - val_loss: 0.0153 - val_mean_absolute_error: 0.0872\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0081 - mean_absolute_error: 0.0605 - val_loss: 0.0138 - val_mean_absolute_error: 0.0819\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 0.0076 - mean_absolute_error: 0.0589 - val_loss: 0.0122 - val_mean_absolute_error: 0.0774\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 0.0072 - mean_absolute_error: 0.0578 - val_loss: 0.0128 - val_mean_absolute_error: 0.0796\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.0069 - mean_absolute_error: 0.0567 - val_loss: 0.0149 - val_mean_absolute_error: 0.0865\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0070 - mean_absolute_error: 0.0575 - val_loss: 0.0126 - val_mean_absolute_error: 0.0779\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0064 - mean_absolute_error: 0.0547 - val_loss: 0.0141 - val_mean_absolute_error: 0.0838\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0066 - mean_absolute_error: 0.0553 - val_loss: 0.0189 - val_mean_absolute_error: 0.1024\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0065 - mean_absolute_error: 0.0554 - val_loss: 0.0126 - val_mean_absolute_error: 0.0791\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0065 - mean_absolute_error: 0.0550 - val_loss: 0.0169 - val_mean_absolute_error: 0.0945\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0062 - mean_absolute_error: 0.0537 - val_loss: 0.0125 - val_mean_absolute_error: 0.0780\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0057 - mean_absolute_error: 0.0518 - val_loss: 0.0133 - val_mean_absolute_error: 0.0814\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0056 - mean_absolute_error: 0.0516 - val_loss: 0.0142 - val_mean_absolute_error: 0.0852\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0054 - mean_absolute_error: 0.0508 - val_loss: 0.0132 - val_mean_absolute_error: 0.0808\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0051 - mean_absolute_error: 0.0496 - val_loss: 0.0151 - val_mean_absolute_error: 0.0878\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0051 - mean_absolute_error: 0.0496 - val_loss: 0.0153 - val_mean_absolute_error: 0.0886\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0051 - mean_absolute_error: 0.0496 - val_loss: 0.0141 - val_mean_absolute_error: 0.0845\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0049 - mean_absolute_error: 0.0486 - val_loss: 0.0151 - val_mean_absolute_error: 0.0879\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0048 - mean_absolute_error: 0.0481 - val_loss: 0.0149 - val_mean_absolute_error: 0.0871\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0052 - mean_absolute_error: 0.0495 - val_loss: 0.0142 - val_mean_absolute_error: 0.0841\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0059 - mean_absolute_error: 0.0525 - val_loss: 0.0144 - val_mean_absolute_error: 0.0849\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0049 - mean_absolute_error: 0.0485 - val_loss: 0.0151 - val_mean_absolute_error: 0.0886\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0047 - mean_absolute_error: 0.0477 - val_loss: 0.0169 - val_mean_absolute_error: 0.0932\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0047 - mean_absolute_error: 0.0478 - val_loss: 0.0140 - val_mean_absolute_error: 0.0830\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0046 - mean_absolute_error: 0.0474 - val_loss: 0.0144 - val_mean_absolute_error: 0.0852\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0045 - mean_absolute_error: 0.0469 - val_loss: 0.0166 - val_mean_absolute_error: 0.0936\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0043 - mean_absolute_error: 0.0460 - val_loss: 0.0150 - val_mean_absolute_error: 0.0870\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0041 - mean_absolute_error: 0.0450 - val_loss: 0.0154 - val_mean_absolute_error: 0.0882\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0044 - mean_absolute_error: 0.0462 - val_loss: 0.0144 - val_mean_absolute_error: 0.0849\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0040 - mean_absolute_error: 0.0448 - val_loss: 0.0158 - val_mean_absolute_error: 0.0903\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0039 - mean_absolute_error: 0.0438 - val_loss: 0.0179 - val_mean_absolute_error: 0.0979\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0041 - mean_absolute_error: 0.0448 - val_loss: 0.0150 - val_mean_absolute_error: 0.0876\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0039 - mean_absolute_error: 0.0441 - val_loss: 0.0158 - val_mean_absolute_error: 0.0898\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0038 - mean_absolute_error: 0.0433 - val_loss: 0.0134 - val_mean_absolute_error: 0.0819\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0037 - mean_absolute_error: 0.0431 - val_loss: 0.0161 - val_mean_absolute_error: 0.0906\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0038 - mean_absolute_error: 0.0431 - val_loss: 0.0155 - val_mean_absolute_error: 0.0883\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0036 - mean_absolute_error: 0.0425 - val_loss: 0.0161 - val_mean_absolute_error: 0.0911\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0036 - mean_absolute_error: 0.0425 - val_loss: 0.0144 - val_mean_absolute_error: 0.0845\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0043 - mean_absolute_error: 0.0454 - val_loss: 0.0161 - val_mean_absolute_error: 0.0903\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0036 - mean_absolute_error: 0.0423 - val_loss: 0.0173 - val_mean_absolute_error: 0.0946\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0034 - mean_absolute_error: 0.0413 - val_loss: 0.0157 - val_mean_absolute_error: 0.0895\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0033 - mean_absolute_error: 0.0410 - val_loss: 0.0168 - val_mean_absolute_error: 0.0934\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0035 - mean_absolute_error: 0.0417 - val_loss: 0.0178 - val_mean_absolute_error: 0.0968\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0034 - mean_absolute_error: 0.0410 - val_loss: 0.0167 - val_mean_absolute_error: 0.0918\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0034 - mean_absolute_error: 0.0412 - val_loss: 0.0163 - val_mean_absolute_error: 0.0917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0032 - mean_absolute_error: 0.0403 - val_loss: 0.0161 - val_mean_absolute_error: 0.0901\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0032 - mean_absolute_error: 0.0403 - val_loss: 0.0155 - val_mean_absolute_error: 0.0882\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0032 - mean_absolute_error: 0.0401 - val_loss: 0.0167 - val_mean_absolute_error: 0.0924\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0031 - mean_absolute_error: 0.0399 - val_loss: 0.0146 - val_mean_absolute_error: 0.0849\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0031 - mean_absolute_error: 0.0398 - val_loss: 0.0174 - val_mean_absolute_error: 0.0948\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.0030 - mean_absolute_error: 0.0392 - val_loss: 0.0164 - val_mean_absolute_error: 0.0917\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0031 - mean_absolute_error: 0.0398 - val_loss: 0.0168 - val_mean_absolute_error: 0.0931\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0030 - mean_absolute_error: 0.0392 - val_loss: 0.0160 - val_mean_absolute_error: 0.0906\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0030 - mean_absolute_error: 0.0392 - val_loss: 0.0180 - val_mean_absolute_error: 0.0970\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0030 - mean_absolute_error: 0.0389 - val_loss: 0.0178 - val_mean_absolute_error: 0.0972\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0029 - mean_absolute_error: 0.0384 - val_loss: 0.0174 - val_mean_absolute_error: 0.0943\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0035 - mean_absolute_error: 0.0416 - val_loss: 0.0155 - val_mean_absolute_error: 0.0883\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0029 - mean_absolute_error: 0.0386 - val_loss: 0.0177 - val_mean_absolute_error: 0.0959\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0031 - mean_absolute_error: 0.0393 - val_loss: 0.0177 - val_mean_absolute_error: 0.0956\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0028 - mean_absolute_error: 0.0377 - val_loss: 0.0179 - val_mean_absolute_error: 0.0967\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 0.0028 - mean_absolute_error: 0.0379 - val_loss: 0.0189 - val_mean_absolute_error: 0.0999\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 0.0028 - mean_absolute_error: 0.0378 - val_loss: 0.0177 - val_mean_absolute_error: 0.0958\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0028 - mean_absolute_error: 0.0379 - val_loss: 0.0162 - val_mean_absolute_error: 0.0907\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0028 - mean_absolute_error: 0.0378 - val_loss: 0.0174 - val_mean_absolute_error: 0.0947\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0027 - mean_absolute_error: 0.0372 - val_loss: 0.0190 - val_mean_absolute_error: 0.1001\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0027 - mean_absolute_error: 0.0368 - val_loss: 0.0175 - val_mean_absolute_error: 0.0951\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0027 - mean_absolute_error: 0.0373 - val_loss: 0.0176 - val_mean_absolute_error: 0.0953\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0026 - mean_absolute_error: 0.0369 - val_loss: 0.0176 - val_mean_absolute_error: 0.0950\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0026 - mean_absolute_error: 0.0366 - val_loss: 0.0168 - val_mean_absolute_error: 0.0920\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0189 - val_mean_absolute_error: 0.0996\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0028 - mean_absolute_error: 0.0376 - val_loss: 0.0172 - val_mean_absolute_error: 0.0936\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0180 - val_mean_absolute_error: 0.0964\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0165 - val_mean_absolute_error: 0.0914\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0026 - mean_absolute_error: 0.0366 - val_loss: 0.0167 - val_mean_absolute_error: 0.0923\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 0.0025 - mean_absolute_error: 0.0360 - val_loss: 0.0214 - val_mean_absolute_error: 0.1069\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0025 - mean_absolute_error: 0.0361 - val_loss: 0.0173 - val_mean_absolute_error: 0.0936\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 6s 79ms/step - loss: 0.0026 - mean_absolute_error: 0.0361 - val_loss: 0.0180 - val_mean_absolute_error: 0.0958\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.0174 - val_mean_absolute_error: 0.0945\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.0025 - mean_absolute_error: 0.0356 - val_loss: 0.0177 - val_mean_absolute_error: 0.0952\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.0178 - val_mean_absolute_error: 0.0947\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.0024 - mean_absolute_error: 0.0353 - val_loss: 0.0172 - val_mean_absolute_error: 0.0942\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0025 - mean_absolute_error: 0.0356 - val_loss: 0.0179 - val_mean_absolute_error: 0.0960\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 0.0031 - mean_absolute_error: 0.0393 - val_loss: 0.0171 - val_mean_absolute_error: 0.0931\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 4s 50ms/step - loss: 0.0025 - mean_absolute_error: 0.0357 - val_loss: 0.0173 - val_mean_absolute_error: 0.0934\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 0.0024 - mean_absolute_error: 0.0351 - val_loss: 0.0166 - val_mean_absolute_error: 0.0910\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0024 - mean_absolute_error: 0.0352 - val_loss: 0.0172 - val_mean_absolute_error: 0.0932\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0025 - mean_absolute_error: 0.0355 - val_loss: 0.0167 - val_mean_absolute_error: 0.0915\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0024 - mean_absolute_error: 0.0348 - val_loss: 0.0172 - val_mean_absolute_error: 0.0931\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.0024 - mean_absolute_error: 0.0348 - val_loss: 0.0177 - val_mean_absolute_error: 0.0950\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.0024 - mean_absolute_error: 0.0348 - val_loss: 0.0180 - val_mean_absolute_error: 0.0959\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.0023 - mean_absolute_error: 0.0346 - val_loss: 0.0169 - val_mean_absolute_error: 0.0925\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 0.0023 - mean_absolute_error: 0.0346 - val_loss: 0.0177 - val_mean_absolute_error: 0.0944\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 5s 75ms/step - loss: 0.0024 - mean_absolute_error: 0.0349 - val_loss: 0.0194 - val_mean_absolute_error: 0.1003\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 5s 68ms/step - loss: 0.0026 - mean_absolute_error: 0.0360 - val_loss: 0.0167 - val_mean_absolute_error: 0.0918\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 0.0023 - mean_absolute_error: 0.0343 - val_loss: 0.0180 - val_mean_absolute_error: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27b6a6f4ed0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3953bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "LSTM_pred = LSTM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0766f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_LSTM = mse(Y_test, LSTM_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a09aec",
   "metadata": {},
   "source": [
    "## Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5502e50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 97)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef043cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.iloc[:-256, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33137314",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data.iloc[-256:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43c02d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk = pd.DataFrame(index=test.index, columns = test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e3c17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(test.columns)):\n",
    "    prev_val = train.iloc[-1, j]\n",
    "    st_dev = train.std().iloc[j]\n",
    "    for i in range(len(test)):\n",
    "        new_val = max(1e-16, prev_val + np.random.normal(0, st_dev, 1))\n",
    "        random_walk.iloc[i, j] = new_val\n",
    "        prev_val = new_val       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d45fba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL.O</th>\n",
       "      <th>MSFT.O</th>\n",
       "      <th>GOOGL.O</th>\n",
       "      <th>AMZN.O</th>\n",
       "      <th>NVDA.O</th>\n",
       "      <th>META.O</th>\n",
       "      <th>BRKb</th>\n",
       "      <th>TSLA.O</th>\n",
       "      <th>LLY</th>\n",
       "      <th>V</th>\n",
       "      <th>...</th>\n",
       "      <th>MDLZ.O</th>\n",
       "      <th>LRCX.O</th>\n",
       "      <th>REGN.O</th>\n",
       "      <th>AMT</th>\n",
       "      <th>PGR</th>\n",
       "      <th>ADP.O</th>\n",
       "      <th>ETN</th>\n",
       "      <th>MMC</th>\n",
       "      <th>ADI.O</th>\n",
       "      <th>CB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12/22/2022</th>\n",
       "      <td>0.5308013084284158</td>\n",
       "      <td>0.581213312408674</td>\n",
       "      <td>0.35390493560553926</td>\n",
       "      <td>0.04014282681420189</td>\n",
       "      <td>0.6601561716598614</td>\n",
       "      <td>0.5875696041917041</td>\n",
       "      <td>0.22285082294354436</td>\n",
       "      <td>0.8325111706989194</td>\n",
       "      <td>0.001088080106447592</td>\n",
       "      <td>0.24212943259588465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15206978783627045</td>\n",
       "      <td>0.5906377897331915</td>\n",
       "      <td>0.13118269868085922</td>\n",
       "      <td>0.33893661537578884</td>\n",
       "      <td>0.12021905232069852</td>\n",
       "      <td>0.044161289685478666</td>\n",
       "      <td>0.0930576501217634</td>\n",
       "      <td>0.18202398047927715</td>\n",
       "      <td>0.44250808979463935</td>\n",
       "      <td>0.11132905296271052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/23/2022</th>\n",
       "      <td>0.4552631323979675</td>\n",
       "      <td>0.6545369947777329</td>\n",
       "      <td>0.2191772373385627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9390176996340265</td>\n",
       "      <td>0.5706513508184726</td>\n",
       "      <td>0.22994066169922903</td>\n",
       "      <td>1.0412580562837628</td>\n",
       "      <td>0.027964681183265984</td>\n",
       "      <td>0.22924077663203687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0315831761825424</td>\n",
       "      <td>0.6518168354809951</td>\n",
       "      <td>0.14404493453132827</td>\n",
       "      <td>0.2332797172506227</td>\n",
       "      <td>0.07445096727557562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.59476123452052e-05</td>\n",
       "      <td>0.17809360612136274</td>\n",
       "      <td>0.37690438383562047</td>\n",
       "      <td>0.08731733006560924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/27/2022</th>\n",
       "      <td>0.39004578459978256</td>\n",
       "      <td>0.7083127873866241</td>\n",
       "      <td>0.2994306702648804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9460886262783498</td>\n",
       "      <td>0.6602174991509789</td>\n",
       "      <td>0.3734016206570201</td>\n",
       "      <td>0.9929092163650164</td>\n",
       "      <td>0.01412603408863552</td>\n",
       "      <td>0.2984562295111538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028740993922020022</td>\n",
       "      <td>0.5829745299432727</td>\n",
       "      <td>0.046536986242659256</td>\n",
       "      <td>0.21325492102109125</td>\n",
       "      <td>0.12924027710717587</td>\n",
       "      <td>0.03438315472673831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1347954353957727</td>\n",
       "      <td>0.10444034189048951</td>\n",
       "      <td>0.16553845459443295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/28/2022</th>\n",
       "      <td>0.25755826040368807</td>\n",
       "      <td>0.6323178317039138</td>\n",
       "      <td>0.21168843065986243</td>\n",
       "      <td>0.10030564162725504</td>\n",
       "      <td>1.3426347438018467</td>\n",
       "      <td>0.31113213352976815</td>\n",
       "      <td>0.5204462241547355</td>\n",
       "      <td>1.1793836241166373</td>\n",
       "      <td>0.014459449745997794</td>\n",
       "      <td>0.15667020412092186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03685820415593621</td>\n",
       "      <td>0.6186298719987813</td>\n",
       "      <td>0.20797778238838635</td>\n",
       "      <td>0.20201601218436044</td>\n",
       "      <td>0.07331504179438633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12079968688066511</td>\n",
       "      <td>0.2087612464212308</td>\n",
       "      <td>0.3039473585619098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/29/2022</th>\n",
       "      <td>0.3644166641464119</td>\n",
       "      <td>0.6456166990971235</td>\n",
       "      <td>0.11806869288291627</td>\n",
       "      <td>0.23120661342023147</td>\n",
       "      <td>1.2187081249932616</td>\n",
       "      <td>0.10715862759601294</td>\n",
       "      <td>0.6285995749776281</td>\n",
       "      <td>1.6626269258821997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07881098065545443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5109442764487127</td>\n",
       "      <td>0.10442893746071073</td>\n",
       "      <td>0.23216924677821466</td>\n",
       "      <td>0.14159794774875972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026399237598549848</td>\n",
       "      <td>0.10466673805242818</td>\n",
       "      <td>0.4441454078761582</td>\n",
       "      <td>0.5793414765485094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/22/2023</th>\n",
       "      <td>3.053033006410532</td>\n",
       "      <td>1.1319151199836233</td>\n",
       "      <td>0.8195999073841094</td>\n",
       "      <td>1.3525169723714257</td>\n",
       "      <td>3.3395338160126338</td>\n",
       "      <td>5.185797210259547</td>\n",
       "      <td>0.625178789791912</td>\n",
       "      <td>0.8120171083764014</td>\n",
       "      <td>1.5178481362798617</td>\n",
       "      <td>0.8698551716905614</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9407508777143154</td>\n",
       "      <td>1.6915484530837075</td>\n",
       "      <td>0.9386059499691981</td>\n",
       "      <td>2.2284040408971664</td>\n",
       "      <td>0.679496927947781</td>\n",
       "      <td>2.458136221722683</td>\n",
       "      <td>3.6610726418880204</td>\n",
       "      <td>3.147677774491358</td>\n",
       "      <td>1.7498229238111396</td>\n",
       "      <td>0.620633912893956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/26/2023</th>\n",
       "      <td>2.7081497461759167</td>\n",
       "      <td>1.0566515203445124</td>\n",
       "      <td>0.8900633605041214</td>\n",
       "      <td>1.303531737705169</td>\n",
       "      <td>3.656666028239712</td>\n",
       "      <td>4.699624702445211</td>\n",
       "      <td>0.7773413930139922</td>\n",
       "      <td>0.5017373436597117</td>\n",
       "      <td>1.437208600678444</td>\n",
       "      <td>0.7704808134872916</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8325941256878227</td>\n",
       "      <td>1.6140678537904607</td>\n",
       "      <td>0.9180632377119009</td>\n",
       "      <td>2.1620643694825237</td>\n",
       "      <td>0.6200776595629763</td>\n",
       "      <td>2.638726661006295</td>\n",
       "      <td>3.5842360993539777</td>\n",
       "      <td>2.9437498979708296</td>\n",
       "      <td>1.9016292794531777</td>\n",
       "      <td>0.5594459350568284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/27/2023</th>\n",
       "      <td>2.9473910345732843</td>\n",
       "      <td>1.1262707843820643</td>\n",
       "      <td>0.6723001132517057</td>\n",
       "      <td>1.2320293957755613</td>\n",
       "      <td>3.882569372991375</td>\n",
       "      <td>4.362531607165873</td>\n",
       "      <td>0.7934677666167729</td>\n",
       "      <td>0.48308422337501716</td>\n",
       "      <td>1.331219763452801</td>\n",
       "      <td>0.6268828632579379</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8651153200905846</td>\n",
       "      <td>1.2279394931699774</td>\n",
       "      <td>0.7429772965017831</td>\n",
       "      <td>2.044247396377132</td>\n",
       "      <td>0.5991892731110078</td>\n",
       "      <td>2.5084978881632396</td>\n",
       "      <td>3.437032080522848</td>\n",
       "      <td>2.951472205290049</td>\n",
       "      <td>1.9948149680855625</td>\n",
       "      <td>0.737109756784891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/28/2023</th>\n",
       "      <td>2.992624809949351</td>\n",
       "      <td>1.3383322146609569</td>\n",
       "      <td>0.7298563523322956</td>\n",
       "      <td>1.2004712068966632</td>\n",
       "      <td>4.056237127462498</td>\n",
       "      <td>4.396290758447743</td>\n",
       "      <td>0.8802694313170485</td>\n",
       "      <td>0.48571365476736394</td>\n",
       "      <td>1.2623821277091727</td>\n",
       "      <td>0.6275709700434918</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9138759543799644</td>\n",
       "      <td>1.3399646132094072</td>\n",
       "      <td>0.6497146952714914</td>\n",
       "      <td>2.108066046816477</td>\n",
       "      <td>0.6386681737657403</td>\n",
       "      <td>2.5616442712986007</td>\n",
       "      <td>3.5061512997100848</td>\n",
       "      <td>2.862797456833753</td>\n",
       "      <td>1.9288527417521437</td>\n",
       "      <td>0.6068075160543933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/29/2023</th>\n",
       "      <td>2.8636414844795137</td>\n",
       "      <td>1.3629799076264664</td>\n",
       "      <td>0.7573263431723188</td>\n",
       "      <td>1.133132748842512</td>\n",
       "      <td>4.412702989850627</td>\n",
       "      <td>4.424594900452215</td>\n",
       "      <td>0.9181347930538916</td>\n",
       "      <td>0.7559371845554299</td>\n",
       "      <td>1.1577601628470082</td>\n",
       "      <td>0.640951565992905</td>\n",
       "      <td>...</td>\n",
       "      <td>3.068430495169848</td>\n",
       "      <td>1.2181677410340392</td>\n",
       "      <td>0.6665449871222873</td>\n",
       "      <td>2.1582570960736667</td>\n",
       "      <td>0.6909847517050342</td>\n",
       "      <td>2.3876397112211687</td>\n",
       "      <td>3.4112178119829673</td>\n",
       "      <td>2.8386238971751228</td>\n",
       "      <td>1.956896529251639</td>\n",
       "      <td>0.60979118953173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         AAPL.O              MSFT.O              GOOGL.O  \\\n",
       "Date                                                                       \n",
       "12/22/2022   0.5308013084284158   0.581213312408674  0.35390493560553926   \n",
       "12/23/2022   0.4552631323979675  0.6545369947777329   0.2191772373385627   \n",
       "12/27/2022  0.39004578459978256  0.7083127873866241   0.2994306702648804   \n",
       "12/28/2022  0.25755826040368807  0.6323178317039138  0.21168843065986243   \n",
       "12/29/2022   0.3644166641464119  0.6456166990971235  0.11806869288291627   \n",
       "...                         ...                 ...                  ...   \n",
       "12/22/2023    3.053033006410532  1.1319151199836233   0.8195999073841094   \n",
       "12/26/2023   2.7081497461759167  1.0566515203445124   0.8900633605041214   \n",
       "12/27/2023   2.9473910345732843  1.1262707843820643   0.6723001132517057   \n",
       "12/28/2023    2.992624809949351  1.3383322146609569   0.7298563523322956   \n",
       "12/29/2023   2.8636414844795137  1.3629799076264664   0.7573263431723188   \n",
       "\n",
       "                         AMZN.O              NVDA.O               META.O  \\\n",
       "Date                                                                       \n",
       "12/22/2022  0.04014282681420189  0.6601561716598614   0.5875696041917041   \n",
       "12/23/2022                  0.0  0.9390176996340265   0.5706513508184726   \n",
       "12/27/2022                  0.0  0.9460886262783498   0.6602174991509789   \n",
       "12/28/2022  0.10030564162725504  1.3426347438018467  0.31113213352976815   \n",
       "12/29/2022  0.23120661342023147  1.2187081249932616  0.10715862759601294   \n",
       "...                         ...                 ...                  ...   \n",
       "12/22/2023   1.3525169723714257  3.3395338160126338    5.185797210259547   \n",
       "12/26/2023    1.303531737705169   3.656666028239712    4.699624702445211   \n",
       "12/27/2023   1.2320293957755613   3.882569372991375    4.362531607165873   \n",
       "12/28/2023   1.2004712068966632   4.056237127462498    4.396290758447743   \n",
       "12/29/2023    1.133132748842512   4.412702989850627    4.424594900452215   \n",
       "\n",
       "                           BRKb               TSLA.O                   LLY  \\\n",
       "Date                                                                         \n",
       "12/22/2022  0.22285082294354436   0.8325111706989194  0.001088080106447592   \n",
       "12/23/2022  0.22994066169922903   1.0412580562837628  0.027964681183265984   \n",
       "12/27/2022   0.3734016206570201   0.9929092163650164   0.01412603408863552   \n",
       "12/28/2022   0.5204462241547355   1.1793836241166373  0.014459449745997794   \n",
       "12/29/2022   0.6285995749776281   1.6626269258821997                   0.0   \n",
       "...                         ...                  ...                   ...   \n",
       "12/22/2023    0.625178789791912   0.8120171083764014    1.5178481362798617   \n",
       "12/26/2023   0.7773413930139922   0.5017373436597117     1.437208600678444   \n",
       "12/27/2023   0.7934677666167729  0.48308422337501716     1.331219763452801   \n",
       "12/28/2023   0.8802694313170485  0.48571365476736394    1.2623821277091727   \n",
       "12/29/2023   0.9181347930538916   0.7559371845554299    1.1577601628470082   \n",
       "\n",
       "                              V  ...                MDLZ.O  \\\n",
       "Date                             ...                         \n",
       "12/22/2022  0.24212943259588465  ...   0.15206978783627045   \n",
       "12/23/2022  0.22924077663203687  ...    0.0315831761825424   \n",
       "12/27/2022   0.2984562295111538  ...  0.028740993922020022   \n",
       "12/28/2022  0.15667020412092186  ...   0.03685820415593621   \n",
       "12/29/2022  0.07881098065545443  ...                   0.0   \n",
       "...                         ...  ...                   ...   \n",
       "12/22/2023   0.8698551716905614  ...    2.9407508777143154   \n",
       "12/26/2023   0.7704808134872916  ...    2.8325941256878227   \n",
       "12/27/2023   0.6268828632579379  ...    2.8651153200905846   \n",
       "12/28/2023   0.6275709700434918  ...    2.9138759543799644   \n",
       "12/29/2023    0.640951565992905  ...     3.068430495169848   \n",
       "\n",
       "                        LRCX.O                REGN.O                  AMT  \\\n",
       "Date                                                                        \n",
       "12/22/2022  0.5906377897331915   0.13118269868085922  0.33893661537578884   \n",
       "12/23/2022  0.6518168354809951   0.14404493453132827   0.2332797172506227   \n",
       "12/27/2022  0.5829745299432727  0.046536986242659256  0.21325492102109125   \n",
       "12/28/2022  0.6186298719987813   0.20797778238838635  0.20201601218436044   \n",
       "12/29/2022  0.5109442764487127   0.10442893746071073  0.23216924677821466   \n",
       "...                        ...                   ...                  ...   \n",
       "12/22/2023  1.6915484530837075    0.9386059499691981   2.2284040408971664   \n",
       "12/26/2023  1.6140678537904607    0.9180632377119009   2.1620643694825237   \n",
       "12/27/2023  1.2279394931699774    0.7429772965017831    2.044247396377132   \n",
       "12/28/2023  1.3399646132094072    0.6497146952714914    2.108066046816477   \n",
       "12/29/2023  1.2181677410340392    0.6665449871222873   2.1582570960736667   \n",
       "\n",
       "                            PGR                 ADP.O                   ETN  \\\n",
       "Date                                                                          \n",
       "12/22/2022  0.12021905232069852  0.044161289685478666    0.0930576501217634   \n",
       "12/23/2022  0.07445096727557562                   0.0  8.59476123452052e-05   \n",
       "12/27/2022  0.12924027710717587   0.03438315472673831                   0.0   \n",
       "12/28/2022  0.07331504179438633                   0.0                   0.0   \n",
       "12/29/2022  0.14159794774875972                   0.0  0.026399237598549848   \n",
       "...                         ...                   ...                   ...   \n",
       "12/22/2023    0.679496927947781     2.458136221722683    3.6610726418880204   \n",
       "12/26/2023   0.6200776595629763     2.638726661006295    3.5842360993539777   \n",
       "12/27/2023   0.5991892731110078    2.5084978881632396     3.437032080522848   \n",
       "12/28/2023   0.6386681737657403    2.5616442712986007    3.5061512997100848   \n",
       "12/29/2023   0.6909847517050342    2.3876397112211687    3.4112178119829673   \n",
       "\n",
       "                            MMC                ADI.O                   CB  \n",
       "Date                                                                       \n",
       "12/22/2022  0.18202398047927715  0.44250808979463935  0.11132905296271052  \n",
       "12/23/2022  0.17809360612136274  0.37690438383562047  0.08731733006560924  \n",
       "12/27/2022   0.1347954353957727  0.10444034189048951  0.16553845459443295  \n",
       "12/28/2022  0.12079968688066511   0.2087612464212308   0.3039473585619098  \n",
       "12/29/2022  0.10466673805242818   0.4441454078761582   0.5793414765485094  \n",
       "...                         ...                  ...                  ...  \n",
       "12/22/2023    3.147677774491358   1.7498229238111396    0.620633912893956  \n",
       "12/26/2023   2.9437498979708296   1.9016292794531777   0.5594459350568284  \n",
       "12/27/2023    2.951472205290049   1.9948149680855625    0.737109756784891  \n",
       "12/28/2023    2.862797456833753   1.9288527417521437   0.6068075160543933  \n",
       "12/29/2023   2.8386238971751228    1.956896529251639     0.60979118953173  \n",
       "\n",
       "[256 rows x 97 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bbeaa259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_RW = mean_squared_error(np.squeeze(Y_test, axis=1), random_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f3427f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0078836195"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07e0fa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009487135"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64e1ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0346740112659316"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fada6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65c993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0ba93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8ef98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632436c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9ead6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf986002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37910aaa",
   "metadata": {},
   "source": [
    "Train, test, and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "44d08881",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_80 = int(len(data.index) * .8)\n",
    "q_90 = int(len(data.index) * .9)\n",
    "\n",
    "train, val, test =  data[:q_80], data[q_80:q_90], data[q_90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce2b8c",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4c755",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6fcf339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b76637c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scaler.transform(train)\n",
    "scaled_val = scaler.transform(val)\n",
    "scaled_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c1f76c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "length=21\n",
    "batch_size = 1024\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length = length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c656f41",
   "metadata": {},
   "source": [
    "Model buliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bb277640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_28 (LSTM)              (None, 100)               79200     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 97)                9797      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88997 (347.64 KB)\n",
      "Trainable params: 88997 (347.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras import activations\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(length, scaled_train.shape[1])))\n",
    "model.add(Dense(scaled_train.shape[1]))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0df4b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=1)\n",
    "validation_generator = TimeseriesGenerator(scaled_val, scaled_val, \n",
    "                                           length=length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd6cbe",
   "metadata": {},
   "source": [
    "Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1f0e1d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anato\\AppData\\Local\\Temp\\ipykernel_476\\2432282585.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 278ms/step - loss: 0.0059 - val_loss: 0.0102\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0058 - val_loss: 0.0101\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 1s 153ms/step - loss: 0.0056 - val_loss: 0.0100\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 1s 164ms/step - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 1s 149ms/step - loss: 0.0054 - val_loss: 0.0096\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0053 - val_loss: 0.0096\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 1s 164ms/step - loss: 0.0052 - val_loss: 0.0097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e136aadc10>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator, epochs=100,\n",
    "                   validation_data=validation_generator,\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead8d33",
   "metadata": {},
   "source": [
    "Recursive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9c74a7c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 791ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "n_features = scaled_train.shape[1]\n",
    "LSTM_pred = []\n",
    "\n",
    "first_eval_batch = scaled_train[-length:]\n",
    "current_batch = first_eval_batch.reshape((1, length, n_features))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    LSTM_pred.append(current_pred)\n",
    "    current_batch = np.append(current_batch[:,1:,:], [[current_pred]], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b4402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "43f6a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_pred = scaler.inverse_transform(LSTM_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3e468e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_pred = pd.DataFrame(data=LSTM_pred, columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "65bfbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "962ea557",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_LSTM = mse(test, LSTM_pred).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

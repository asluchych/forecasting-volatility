{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6963ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7078242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310021a",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86c03369",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('daily_rv_daily_data2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c3958",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae6a580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_Y(df, window_size=21):\n",
    "  df_as_np = df.to_numpy()\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size):\n",
    "    row = [r for r in df_as_np[i:i+window_size]]\n",
    "    X.append(row)\n",
    "    label = [df_as_np[i+window_size]]\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f57cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df_to_X_Y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8958af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_80 = int(len(data.index) * .8)\n",
    "q_90 = int(len(data.index) * .9)\n",
    "\n",
    "X_train, Y_train =  X[:q_80], Y[:q_80]\n",
    "\n",
    "X_val, Y_val =  X[q_80:q_90], Y[q_80:q_90]\n",
    "X_test, Y_test =  X[q_90:], Y[q_90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c82a0a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 21, 100)           9800      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 21, 100)           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 21, 100)           10100     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 21, 100)           0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 21, 97)            9797      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29697 (116.00 KB)\n",
      "Trainable params: 29697 (116.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from keras import activations\n",
    "\n",
    "MLP = Sequential([layers.Input((21, 97)),\n",
    "                    layers.Dense(100),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(100),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(97, activation=activations.relu)])\n",
    "\n",
    "MLP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "798a84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.compile(loss='mse', \n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "08a27c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0530 - val_loss: 0.0216 - val_mean_absolute_error: 0.1128\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0268 - val_mean_absolute_error: 0.1302\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0212 - val_mean_absolute_error: 0.1111\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0542 - val_loss: 0.0212 - val_mean_absolute_error: 0.1113\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0194 - val_mean_absolute_error: 0.1054\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0536 - val_loss: 0.0243 - val_mean_absolute_error: 0.1215\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0212 - val_mean_absolute_error: 0.1114\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0207 - val_mean_absolute_error: 0.1100\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0213 - val_mean_absolute_error: 0.1112\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0224 - val_mean_absolute_error: 0.1154\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0217 - val_mean_absolute_error: 0.1128\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0235 - val_mean_absolute_error: 0.1197\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0067 - mean_absolute_error: 0.0550 - val_loss: 0.0229 - val_mean_absolute_error: 0.1157\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0536 - val_loss: 0.0203 - val_mean_absolute_error: 0.1086\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0201 - val_mean_absolute_error: 0.1073\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0188 - val_mean_absolute_error: 0.1035\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0541 - val_loss: 0.0209 - val_mean_absolute_error: 0.1101\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0247 - val_mean_absolute_error: 0.1221\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0199 - val_mean_absolute_error: 0.1071\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0280 - val_mean_absolute_error: 0.1325\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0241 - val_mean_absolute_error: 0.1207\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0531 - val_loss: 0.0221 - val_mean_absolute_error: 0.1139\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0200 - val_mean_absolute_error: 0.1067\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0541 - val_loss: 0.0199 - val_mean_absolute_error: 0.1067\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0202 - val_mean_absolute_error: 0.1078\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0266 - val_mean_absolute_error: 0.1277\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0220 - val_mean_absolute_error: 0.1149\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0218 - val_mean_absolute_error: 0.1137\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0177 - val_mean_absolute_error: 0.0998\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0066 - mean_absolute_error: 0.0552 - val_loss: 0.0203 - val_mean_absolute_error: 0.1084\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0065 - mean_absolute_error: 0.0544 - val_loss: 0.0206 - val_mean_absolute_error: 0.1095\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0213 - val_mean_absolute_error: 0.1119\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0532 - val_loss: 0.0195 - val_mean_absolute_error: 0.1058\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0261 - val_mean_absolute_error: 0.1263\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0240 - val_mean_absolute_error: 0.1200\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0232 - val_mean_absolute_error: 0.1182\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0542 - val_loss: 0.0236 - val_mean_absolute_error: 0.1185\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0540 - val_loss: 0.0234 - val_mean_absolute_error: 0.1190\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0061 - mean_absolute_error: 0.0527 - val_loss: 0.0209 - val_mean_absolute_error: 0.1111\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0214 - val_mean_absolute_error: 0.1121\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0542 - val_loss: 0.0156 - val_mean_absolute_error: 0.0923\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0543 - val_loss: 0.0219 - val_mean_absolute_error: 0.1143\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0190 - val_mean_absolute_error: 0.1041\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0546 - val_loss: 0.0230 - val_mean_absolute_error: 0.1176\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0534 - val_loss: 0.0208 - val_mean_absolute_error: 0.1097\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0265 - val_mean_absolute_error: 0.1279\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0530 - val_loss: 0.0243 - val_mean_absolute_error: 0.1207\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0536 - val_loss: 0.0239 - val_mean_absolute_error: 0.1197\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0205 - val_mean_absolute_error: 0.1089\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0534 - val_loss: 0.0221 - val_mean_absolute_error: 0.1144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0061 - mean_absolute_error: 0.0532 - val_loss: 0.0256 - val_mean_absolute_error: 0.1258\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0539 - val_loss: 0.0291 - val_mean_absolute_error: 0.1358\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0540 - val_loss: 0.0219 - val_mean_absolute_error: 0.1140\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0537 - val_loss: 0.0215 - val_mean_absolute_error: 0.1114\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0540 - val_loss: 0.0239 - val_mean_absolute_error: 0.1194\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0219 - val_mean_absolute_error: 0.1135\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0243 - val_mean_absolute_error: 0.1215\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0240 - val_mean_absolute_error: 0.1203\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0528 - val_loss: 0.0219 - val_mean_absolute_error: 0.1132\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0240 - val_mean_absolute_error: 0.1203\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0304 - val_mean_absolute_error: 0.1402\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0066 - mean_absolute_error: 0.0551 - val_loss: 0.0236 - val_mean_absolute_error: 0.1183\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0064 - mean_absolute_error: 0.0540 - val_loss: 0.0200 - val_mean_absolute_error: 0.1073\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0233 - val_mean_absolute_error: 0.1177\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0064 - mean_absolute_error: 0.0540 - val_loss: 0.0235 - val_mean_absolute_error: 0.1185\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0065 - mean_absolute_error: 0.0550 - val_loss: 0.0217 - val_mean_absolute_error: 0.1125\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.0065 - mean_absolute_error: 0.0542 - val_loss: 0.0192 - val_mean_absolute_error: 0.1042\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0066 - mean_absolute_error: 0.0553 - val_loss: 0.0195 - val_mean_absolute_error: 0.1059\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0065 - mean_absolute_error: 0.0550 - val_loss: 0.0227 - val_mean_absolute_error: 0.1160\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0532 - val_loss: 0.0192 - val_mean_absolute_error: 0.1045\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0254 - val_mean_absolute_error: 0.1246\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0232 - val_mean_absolute_error: 0.1180\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0537 - val_loss: 0.0287 - val_mean_absolute_error: 0.1348\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0210 - val_mean_absolute_error: 0.1105\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 1s 21ms/step - loss: 0.0064 - mean_absolute_error: 0.0538 - val_loss: 0.0223 - val_mean_absolute_error: 0.1148\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0233 - val_mean_absolute_error: 0.1180\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 0.0062 - mean_absolute_error: 0.0531 - val_loss: 0.0227 - val_mean_absolute_error: 0.1159\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.0061 - mean_absolute_error: 0.0531 - val_loss: 0.0263 - val_mean_absolute_error: 0.1266\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0258 - val_mean_absolute_error: 0.1260\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0062 - mean_absolute_error: 0.0536 - val_loss: 0.0268 - val_mean_absolute_error: 0.1289\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 0.0065 - mean_absolute_error: 0.0546 - val_loss: 0.0250 - val_mean_absolute_error: 0.1230\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 2s 24ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0238 - val_mean_absolute_error: 0.1197\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 2s 29ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0170 - val_mean_absolute_error: 0.0971\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 1s 20ms/step - loss: 0.0062 - mean_absolute_error: 0.0533 - val_loss: 0.0263 - val_mean_absolute_error: 0.1269\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0236 - val_mean_absolute_error: 0.1190\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0063 - mean_absolute_error: 0.0538 - val_loss: 0.0221 - val_mean_absolute_error: 0.1142\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0065 - mean_absolute_error: 0.0542 - val_loss: 0.0170 - val_mean_absolute_error: 0.0973\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0538 - val_loss: 0.0193 - val_mean_absolute_error: 0.1045\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.0063 - mean_absolute_error: 0.0534 - val_loss: 0.0175 - val_mean_absolute_error: 0.0988\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0061 - mean_absolute_error: 0.0531 - val_loss: 0.0195 - val_mean_absolute_error: 0.1051\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0062 - mean_absolute_error: 0.0534 - val_loss: 0.0252 - val_mean_absolute_error: 0.1234\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0532 - val_loss: 0.0255 - val_mean_absolute_error: 0.1247\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0066 - mean_absolute_error: 0.0553 - val_loss: 0.0262 - val_mean_absolute_error: 0.1262\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0063 - mean_absolute_error: 0.0539 - val_loss: 0.0188 - val_mean_absolute_error: 0.1030\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0063 - mean_absolute_error: 0.0535 - val_loss: 0.0206 - val_mean_absolute_error: 0.1092\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0542 - val_loss: 0.0236 - val_mean_absolute_error: 0.1182\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0535 - val_loss: 0.0218 - val_mean_absolute_error: 0.1134\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0061 - mean_absolute_error: 0.0530 - val_loss: 0.0234 - val_mean_absolute_error: 0.1186\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.0062 - mean_absolute_error: 0.0530 - val_loss: 0.0220 - val_mean_absolute_error: 0.1136\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 0.0062 - mean_absolute_error: 0.0537 - val_loss: 0.0224 - val_mean_absolute_error: 0.1152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27b022b2b10>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a515bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "MLP_pred = MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0d093dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 21, 97)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c7593d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d159fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_MLP = mse(Y_test, MLP_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00866d3",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19c24663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 21, 100)           79200     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 21, 100)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 21, 100)           80400     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 21, 100)           0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 21, 97)            9797      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169397 (661.71 KB)\n",
      "Trainable params: 169397 (661.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM = Sequential([layers.Input((21, 97)),\n",
    "                    layers.LSTM(100, return_sequences=True),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.LSTM(100, return_sequences=True),\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(97, activation=activations.relu)])\n",
    "\n",
    "LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5983f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM.compile(loss='mse', \n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5687014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70/70 [==============================] - 12s 78ms/step - loss: 0.0256 - mean_absolute_error: 0.1089 - val_loss: 0.0136 - val_mean_absolute_error: 0.0820\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0121 - mean_absolute_error: 0.0733 - val_loss: 0.0129 - val_mean_absolute_error: 0.0784\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0104 - mean_absolute_error: 0.0672 - val_loss: 0.0115 - val_mean_absolute_error: 0.0790\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0098 - mean_absolute_error: 0.0666 - val_loss: 0.0124 - val_mean_absolute_error: 0.0775\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0089 - mean_absolute_error: 0.0630 - val_loss: 0.0124 - val_mean_absolute_error: 0.0774\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0087 - mean_absolute_error: 0.0626 - val_loss: 0.0153 - val_mean_absolute_error: 0.0872\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0081 - mean_absolute_error: 0.0605 - val_loss: 0.0138 - val_mean_absolute_error: 0.0819\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 0.0076 - mean_absolute_error: 0.0589 - val_loss: 0.0122 - val_mean_absolute_error: 0.0774\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 0.0072 - mean_absolute_error: 0.0578 - val_loss: 0.0128 - val_mean_absolute_error: 0.0796\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.0069 - mean_absolute_error: 0.0567 - val_loss: 0.0149 - val_mean_absolute_error: 0.0865\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0070 - mean_absolute_error: 0.0575 - val_loss: 0.0126 - val_mean_absolute_error: 0.0779\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0064 - mean_absolute_error: 0.0547 - val_loss: 0.0141 - val_mean_absolute_error: 0.0838\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0066 - mean_absolute_error: 0.0553 - val_loss: 0.0189 - val_mean_absolute_error: 0.1024\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0065 - mean_absolute_error: 0.0554 - val_loss: 0.0126 - val_mean_absolute_error: 0.0791\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0065 - mean_absolute_error: 0.0550 - val_loss: 0.0169 - val_mean_absolute_error: 0.0945\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0062 - mean_absolute_error: 0.0537 - val_loss: 0.0125 - val_mean_absolute_error: 0.0780\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0057 - mean_absolute_error: 0.0518 - val_loss: 0.0133 - val_mean_absolute_error: 0.0814\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0056 - mean_absolute_error: 0.0516 - val_loss: 0.0142 - val_mean_absolute_error: 0.0852\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0054 - mean_absolute_error: 0.0508 - val_loss: 0.0132 - val_mean_absolute_error: 0.0808\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0051 - mean_absolute_error: 0.0496 - val_loss: 0.0151 - val_mean_absolute_error: 0.0878\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0051 - mean_absolute_error: 0.0496 - val_loss: 0.0153 - val_mean_absolute_error: 0.0886\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0051 - mean_absolute_error: 0.0496 - val_loss: 0.0141 - val_mean_absolute_error: 0.0845\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0049 - mean_absolute_error: 0.0486 - val_loss: 0.0151 - val_mean_absolute_error: 0.0879\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0048 - mean_absolute_error: 0.0481 - val_loss: 0.0149 - val_mean_absolute_error: 0.0871\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0052 - mean_absolute_error: 0.0495 - val_loss: 0.0142 - val_mean_absolute_error: 0.0841\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0059 - mean_absolute_error: 0.0525 - val_loss: 0.0144 - val_mean_absolute_error: 0.0849\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0049 - mean_absolute_error: 0.0485 - val_loss: 0.0151 - val_mean_absolute_error: 0.0886\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0047 - mean_absolute_error: 0.0477 - val_loss: 0.0169 - val_mean_absolute_error: 0.0932\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0047 - mean_absolute_error: 0.0478 - val_loss: 0.0140 - val_mean_absolute_error: 0.0830\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0046 - mean_absolute_error: 0.0474 - val_loss: 0.0144 - val_mean_absolute_error: 0.0852\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0045 - mean_absolute_error: 0.0469 - val_loss: 0.0166 - val_mean_absolute_error: 0.0936\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0043 - mean_absolute_error: 0.0460 - val_loss: 0.0150 - val_mean_absolute_error: 0.0870\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0041 - mean_absolute_error: 0.0450 - val_loss: 0.0154 - val_mean_absolute_error: 0.0882\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0044 - mean_absolute_error: 0.0462 - val_loss: 0.0144 - val_mean_absolute_error: 0.0849\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0040 - mean_absolute_error: 0.0448 - val_loss: 0.0158 - val_mean_absolute_error: 0.0903\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0039 - mean_absolute_error: 0.0438 - val_loss: 0.0179 - val_mean_absolute_error: 0.0979\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0041 - mean_absolute_error: 0.0448 - val_loss: 0.0150 - val_mean_absolute_error: 0.0876\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0039 - mean_absolute_error: 0.0441 - val_loss: 0.0158 - val_mean_absolute_error: 0.0898\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0038 - mean_absolute_error: 0.0433 - val_loss: 0.0134 - val_mean_absolute_error: 0.0819\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0037 - mean_absolute_error: 0.0431 - val_loss: 0.0161 - val_mean_absolute_error: 0.0906\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0038 - mean_absolute_error: 0.0431 - val_loss: 0.0155 - val_mean_absolute_error: 0.0883\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0036 - mean_absolute_error: 0.0425 - val_loss: 0.0161 - val_mean_absolute_error: 0.0911\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0036 - mean_absolute_error: 0.0425 - val_loss: 0.0144 - val_mean_absolute_error: 0.0845\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0043 - mean_absolute_error: 0.0454 - val_loss: 0.0161 - val_mean_absolute_error: 0.0903\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0036 - mean_absolute_error: 0.0423 - val_loss: 0.0173 - val_mean_absolute_error: 0.0946\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0034 - mean_absolute_error: 0.0413 - val_loss: 0.0157 - val_mean_absolute_error: 0.0895\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0033 - mean_absolute_error: 0.0410 - val_loss: 0.0168 - val_mean_absolute_error: 0.0934\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0035 - mean_absolute_error: 0.0417 - val_loss: 0.0178 - val_mean_absolute_error: 0.0968\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0034 - mean_absolute_error: 0.0410 - val_loss: 0.0167 - val_mean_absolute_error: 0.0918\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0034 - mean_absolute_error: 0.0412 - val_loss: 0.0163 - val_mean_absolute_error: 0.0917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0032 - mean_absolute_error: 0.0403 - val_loss: 0.0161 - val_mean_absolute_error: 0.0901\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0032 - mean_absolute_error: 0.0403 - val_loss: 0.0155 - val_mean_absolute_error: 0.0882\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0032 - mean_absolute_error: 0.0401 - val_loss: 0.0167 - val_mean_absolute_error: 0.0924\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0031 - mean_absolute_error: 0.0399 - val_loss: 0.0146 - val_mean_absolute_error: 0.0849\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0031 - mean_absolute_error: 0.0398 - val_loss: 0.0174 - val_mean_absolute_error: 0.0948\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.0030 - mean_absolute_error: 0.0392 - val_loss: 0.0164 - val_mean_absolute_error: 0.0917\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0031 - mean_absolute_error: 0.0398 - val_loss: 0.0168 - val_mean_absolute_error: 0.0931\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0030 - mean_absolute_error: 0.0392 - val_loss: 0.0160 - val_mean_absolute_error: 0.0906\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0030 - mean_absolute_error: 0.0392 - val_loss: 0.0180 - val_mean_absolute_error: 0.0970\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0030 - mean_absolute_error: 0.0389 - val_loss: 0.0178 - val_mean_absolute_error: 0.0972\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0029 - mean_absolute_error: 0.0384 - val_loss: 0.0174 - val_mean_absolute_error: 0.0943\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0035 - mean_absolute_error: 0.0416 - val_loss: 0.0155 - val_mean_absolute_error: 0.0883\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0029 - mean_absolute_error: 0.0386 - val_loss: 0.0177 - val_mean_absolute_error: 0.0959\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0031 - mean_absolute_error: 0.0393 - val_loss: 0.0177 - val_mean_absolute_error: 0.0956\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0028 - mean_absolute_error: 0.0377 - val_loss: 0.0179 - val_mean_absolute_error: 0.0967\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 0.0028 - mean_absolute_error: 0.0379 - val_loss: 0.0189 - val_mean_absolute_error: 0.0999\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 0.0028 - mean_absolute_error: 0.0378 - val_loss: 0.0177 - val_mean_absolute_error: 0.0958\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0028 - mean_absolute_error: 0.0379 - val_loss: 0.0162 - val_mean_absolute_error: 0.0907\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0028 - mean_absolute_error: 0.0378 - val_loss: 0.0174 - val_mean_absolute_error: 0.0947\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0027 - mean_absolute_error: 0.0372 - val_loss: 0.0190 - val_mean_absolute_error: 0.1001\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0027 - mean_absolute_error: 0.0368 - val_loss: 0.0175 - val_mean_absolute_error: 0.0951\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0027 - mean_absolute_error: 0.0373 - val_loss: 0.0176 - val_mean_absolute_error: 0.0953\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0026 - mean_absolute_error: 0.0369 - val_loss: 0.0176 - val_mean_absolute_error: 0.0950\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0026 - mean_absolute_error: 0.0366 - val_loss: 0.0168 - val_mean_absolute_error: 0.0920\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0189 - val_mean_absolute_error: 0.0996\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0028 - mean_absolute_error: 0.0376 - val_loss: 0.0172 - val_mean_absolute_error: 0.0936\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0180 - val_mean_absolute_error: 0.0964\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0026 - mean_absolute_error: 0.0364 - val_loss: 0.0165 - val_mean_absolute_error: 0.0914\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0026 - mean_absolute_error: 0.0366 - val_loss: 0.0167 - val_mean_absolute_error: 0.0923\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 3s 48ms/step - loss: 0.0025 - mean_absolute_error: 0.0360 - val_loss: 0.0214 - val_mean_absolute_error: 0.1069\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.0025 - mean_absolute_error: 0.0361 - val_loss: 0.0173 - val_mean_absolute_error: 0.0936\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 6s 79ms/step - loss: 0.0026 - mean_absolute_error: 0.0361 - val_loss: 0.0180 - val_mean_absolute_error: 0.0958\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.0174 - val_mean_absolute_error: 0.0945\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.0025 - mean_absolute_error: 0.0356 - val_loss: 0.0177 - val_mean_absolute_error: 0.0952\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 4s 58ms/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.0178 - val_mean_absolute_error: 0.0947\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.0024 - mean_absolute_error: 0.0353 - val_loss: 0.0172 - val_mean_absolute_error: 0.0942\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0025 - mean_absolute_error: 0.0356 - val_loss: 0.0179 - val_mean_absolute_error: 0.0960\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 0.0031 - mean_absolute_error: 0.0393 - val_loss: 0.0171 - val_mean_absolute_error: 0.0931\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 4s 50ms/step - loss: 0.0025 - mean_absolute_error: 0.0357 - val_loss: 0.0173 - val_mean_absolute_error: 0.0934\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 0.0024 - mean_absolute_error: 0.0351 - val_loss: 0.0166 - val_mean_absolute_error: 0.0910\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0024 - mean_absolute_error: 0.0352 - val_loss: 0.0172 - val_mean_absolute_error: 0.0932\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.0025 - mean_absolute_error: 0.0355 - val_loss: 0.0167 - val_mean_absolute_error: 0.0915\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 3s 46ms/step - loss: 0.0024 - mean_absolute_error: 0.0348 - val_loss: 0.0172 - val_mean_absolute_error: 0.0931\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.0024 - mean_absolute_error: 0.0348 - val_loss: 0.0177 - val_mean_absolute_error: 0.0950\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.0024 - mean_absolute_error: 0.0348 - val_loss: 0.0180 - val_mean_absolute_error: 0.0959\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.0023 - mean_absolute_error: 0.0346 - val_loss: 0.0169 - val_mean_absolute_error: 0.0925\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 0.0023 - mean_absolute_error: 0.0346 - val_loss: 0.0177 - val_mean_absolute_error: 0.0944\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 5s 75ms/step - loss: 0.0024 - mean_absolute_error: 0.0349 - val_loss: 0.0194 - val_mean_absolute_error: 0.1003\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 5s 68ms/step - loss: 0.0026 - mean_absolute_error: 0.0360 - val_loss: 0.0167 - val_mean_absolute_error: 0.0918\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 0.0023 - mean_absolute_error: 0.0343 - val_loss: 0.0180 - val_mean_absolute_error: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27b6a6f4ed0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba85edaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "LSTM_pred = LSTM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff995ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_LSTM = mse(Y_test, LSTM_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf8046",
   "metadata": {},
   "source": [
    "## Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17351505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 97)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2755f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.iloc[:-256, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a6b064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data.iloc[-256:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2e8d6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_pred = pd.DataFrame(index=test.index, columns = test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0728b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(test.columns)):\n",
    "    prev_val = train.iloc[-1, j]\n",
    "    st_dev = train.std().iloc[j]\n",
    "    for i in range(len(test)):\n",
    "        new_val = max(1e-16, prev_val + np.random.normal(0, st_dev, 1))\n",
    "        rw_pred.iloc[i, j] = new_val\n",
    "        prev_val = new_val       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a21880e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL.O</th>\n",
       "      <th>MSFT.O</th>\n",
       "      <th>GOOGL.O</th>\n",
       "      <th>AMZN.O</th>\n",
       "      <th>NVDA.O</th>\n",
       "      <th>META.O</th>\n",
       "      <th>BRKb</th>\n",
       "      <th>TSLA.O</th>\n",
       "      <th>LLY</th>\n",
       "      <th>V</th>\n",
       "      <th>...</th>\n",
       "      <th>MDLZ.O</th>\n",
       "      <th>LRCX.O</th>\n",
       "      <th>REGN.O</th>\n",
       "      <th>AMT</th>\n",
       "      <th>PGR</th>\n",
       "      <th>ADP.O</th>\n",
       "      <th>ETN</th>\n",
       "      <th>MMC</th>\n",
       "      <th>ADI.O</th>\n",
       "      <th>CB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12/22/2022</th>\n",
       "      <td>0.3596978615341253</td>\n",
       "      <td>0.40721217638257423</td>\n",
       "      <td>0.22414987322905616</td>\n",
       "      <td>0.25180160332983503</td>\n",
       "      <td>0.3516497899131491</td>\n",
       "      <td>0.6373805443775048</td>\n",
       "      <td>0.1807834668597527</td>\n",
       "      <td>0.7724302560148639</td>\n",
       "      <td>0.2783565003979402</td>\n",
       "      <td>0.08218742305092158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22324078878444337</td>\n",
       "      <td>0.9112117816397765</td>\n",
       "      <td>0.22467091994047164</td>\n",
       "      <td>0.22911399340818858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2659701601015139</td>\n",
       "      <td>0.1970186852723305</td>\n",
       "      <td>0.24586656277971988</td>\n",
       "      <td>0.4143203164432392</td>\n",
       "      <td>0.05705791978550323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/23/2022</th>\n",
       "      <td>0.5047077054702845</td>\n",
       "      <td>0.2555105134114005</td>\n",
       "      <td>0.10239298641509292</td>\n",
       "      <td>0.2712569778037012</td>\n",
       "      <td>0.4147092641800432</td>\n",
       "      <td>0.7825319323092894</td>\n",
       "      <td>0.05551119178665381</td>\n",
       "      <td>0.9119564700178806</td>\n",
       "      <td>0.5034239236084705</td>\n",
       "      <td>0.17426407175876968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9628471566280784</td>\n",
       "      <td>0.2723971667995879</td>\n",
       "      <td>0.45341374943957635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1792559362102821</td>\n",
       "      <td>0.2460887597431093</td>\n",
       "      <td>0.48942905259908154</td>\n",
       "      <td>0.5059598520309163</td>\n",
       "      <td>0.17947667276977897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/27/2022</th>\n",
       "      <td>0.5330868382634594</td>\n",
       "      <td>0.17581423641842342</td>\n",
       "      <td>0.10769040018752521</td>\n",
       "      <td>0.2499884898337587</td>\n",
       "      <td>0.30724380866879264</td>\n",
       "      <td>0.9276709439758826</td>\n",
       "      <td>0.10699529701363969</td>\n",
       "      <td>1.0621971674937245</td>\n",
       "      <td>0.4190859877513477</td>\n",
       "      <td>0.21241606578114655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06611834976548546</td>\n",
       "      <td>0.8257298987275898</td>\n",
       "      <td>0.38108087876065994</td>\n",
       "      <td>0.5956294331151347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19342556544723746</td>\n",
       "      <td>0.20928144593228293</td>\n",
       "      <td>0.38814570729822295</td>\n",
       "      <td>0.6734032913299469</td>\n",
       "      <td>0.20377703815077491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/28/2022</th>\n",
       "      <td>0.6216762522469449</td>\n",
       "      <td>0.19673909031095638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21139509185122402</td>\n",
       "      <td>0.30418921496222956</td>\n",
       "      <td>0.9320431966710729</td>\n",
       "      <td>0.05827712268428653</td>\n",
       "      <td>1.3914117106322565</td>\n",
       "      <td>0.1499574947947494</td>\n",
       "      <td>0.2201425028732636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052474020052158496</td>\n",
       "      <td>0.9520389332371655</td>\n",
       "      <td>0.5258438787249273</td>\n",
       "      <td>0.8783992995040109</td>\n",
       "      <td>0.10586091678166232</td>\n",
       "      <td>0.21318540363602995</td>\n",
       "      <td>0.28942794853369846</td>\n",
       "      <td>0.4746552733467998</td>\n",
       "      <td>0.4571654651390782</td>\n",
       "      <td>0.15920796494497425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/29/2022</th>\n",
       "      <td>0.633824174797444</td>\n",
       "      <td>0.18557604079247125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20821098456411746</td>\n",
       "      <td>0.5301097048424472</td>\n",
       "      <td>0.8314849065689208</td>\n",
       "      <td>0.006936855074602712</td>\n",
       "      <td>1.685456506642552</td>\n",
       "      <td>0.18366112594290285</td>\n",
       "      <td>0.37435471264176057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1010432041942771</td>\n",
       "      <td>0.9958523928097559</td>\n",
       "      <td>0.7559641431738413</td>\n",
       "      <td>0.917820540512137</td>\n",
       "      <td>0.014742915153796388</td>\n",
       "      <td>0.10000897944490701</td>\n",
       "      <td>0.30366524279918233</td>\n",
       "      <td>0.4109609659934432</td>\n",
       "      <td>0.30746992014966235</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/22/2023</th>\n",
       "      <td>1.4990026967242838</td>\n",
       "      <td>3.2401459259976813</td>\n",
       "      <td>0.7414193099414598</td>\n",
       "      <td>0.6009075771703185</td>\n",
       "      <td>2.5460564786040196</td>\n",
       "      <td>0.01135499596756195</td>\n",
       "      <td>1.7413369858893524</td>\n",
       "      <td>9.514757940424353</td>\n",
       "      <td>2.3715672665429492</td>\n",
       "      <td>0.8543125239697424</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3360840017366213</td>\n",
       "      <td>2.0481780747922236</td>\n",
       "      <td>0.17480712672920365</td>\n",
       "      <td>3.558225305690294</td>\n",
       "      <td>1.0187716000119107</td>\n",
       "      <td>1.4776953860199193</td>\n",
       "      <td>0.5414067073357065</td>\n",
       "      <td>0.16056634179955562</td>\n",
       "      <td>0.11967175932367335</td>\n",
       "      <td>2.566590262648768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/26/2023</th>\n",
       "      <td>1.686851986110891</td>\n",
       "      <td>3.3793085905019464</td>\n",
       "      <td>0.7948185780486524</td>\n",
       "      <td>0.7051192884665655</td>\n",
       "      <td>2.211761126626536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6993971249898112</td>\n",
       "      <td>9.803992028917644</td>\n",
       "      <td>2.064824856259823</td>\n",
       "      <td>0.9347258229110478</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3249366751148233</td>\n",
       "      <td>2.3013085028654516</td>\n",
       "      <td>0.27790394655287876</td>\n",
       "      <td>3.527400301675957</td>\n",
       "      <td>0.8109193657273481</td>\n",
       "      <td>1.5152058037941434</td>\n",
       "      <td>0.4461575431145655</td>\n",
       "      <td>0.08390991022396377</td>\n",
       "      <td>0.13286005617129087</td>\n",
       "      <td>2.6607050384773108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/27/2023</th>\n",
       "      <td>1.744957837831905</td>\n",
       "      <td>3.097218233266659</td>\n",
       "      <td>0.6753683068685802</td>\n",
       "      <td>0.7885254062016848</td>\n",
       "      <td>2.914086307569919</td>\n",
       "      <td>0.028695511685780457</td>\n",
       "      <td>1.6468981653416397</td>\n",
       "      <td>9.419098544337771</td>\n",
       "      <td>1.7343897836243543</td>\n",
       "      <td>0.8761851498454007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2861829390093644</td>\n",
       "      <td>2.1792039204752247</td>\n",
       "      <td>0.06671071520238966</td>\n",
       "      <td>3.527529298041054</td>\n",
       "      <td>0.7891558938096599</td>\n",
       "      <td>1.6041854823635882</td>\n",
       "      <td>0.37339749547366874</td>\n",
       "      <td>0.027719767670564367</td>\n",
       "      <td>0.08834600922862032</td>\n",
       "      <td>2.7355854044809305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/28/2023</th>\n",
       "      <td>1.7183897078317956</td>\n",
       "      <td>3.3650799667847524</td>\n",
       "      <td>0.6699418553597415</td>\n",
       "      <td>0.6588619348813373</td>\n",
       "      <td>2.9897427640754217</td>\n",
       "      <td>0.3114981992853912</td>\n",
       "      <td>1.711962643368723</td>\n",
       "      <td>9.212326146741244</td>\n",
       "      <td>1.765189639711142</td>\n",
       "      <td>0.9244779493256436</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3729052131441728</td>\n",
       "      <td>2.435340923325042</td>\n",
       "      <td>0.1306652014215108</td>\n",
       "      <td>3.4430788404887824</td>\n",
       "      <td>0.6303565888418876</td>\n",
       "      <td>1.407636044733688</td>\n",
       "      <td>0.4156815037033561</td>\n",
       "      <td>0.02995670039482805</td>\n",
       "      <td>0.1270129750934561</td>\n",
       "      <td>2.5085519250989203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/29/2023</th>\n",
       "      <td>1.7746500859264507</td>\n",
       "      <td>3.2305289274978755</td>\n",
       "      <td>0.7956947491701549</td>\n",
       "      <td>0.8179580265439256</td>\n",
       "      <td>2.98437678716954</td>\n",
       "      <td>0.5552111303683664</td>\n",
       "      <td>1.6294990845944446</td>\n",
       "      <td>9.24328712915325</td>\n",
       "      <td>1.6163103932992242</td>\n",
       "      <td>1.146298468163738</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4298348088631252</td>\n",
       "      <td>2.2988366040369717</td>\n",
       "      <td>0.17288690939135803</td>\n",
       "      <td>3.3143783043087227</td>\n",
       "      <td>0.48568170417530243</td>\n",
       "      <td>1.3736756078484456</td>\n",
       "      <td>0.44578946033279204</td>\n",
       "      <td>0.11381677036829892</td>\n",
       "      <td>0.21590034828270727</td>\n",
       "      <td>2.5709913257976362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows  97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        AAPL.O               MSFT.O              GOOGL.O  \\\n",
       "Date                                                                       \n",
       "12/22/2022  0.3596978615341253  0.40721217638257423  0.22414987322905616   \n",
       "12/23/2022  0.5047077054702845   0.2555105134114005  0.10239298641509292   \n",
       "12/27/2022  0.5330868382634594  0.17581423641842342  0.10769040018752521   \n",
       "12/28/2022  0.6216762522469449  0.19673909031095638                  0.0   \n",
       "12/29/2022   0.633824174797444  0.18557604079247125                  0.0   \n",
       "...                        ...                  ...                  ...   \n",
       "12/22/2023  1.4990026967242838   3.2401459259976813   0.7414193099414598   \n",
       "12/26/2023   1.686851986110891   3.3793085905019464   0.7948185780486524   \n",
       "12/27/2023   1.744957837831905    3.097218233266659   0.6753683068685802   \n",
       "12/28/2023  1.7183897078317956   3.3650799667847524   0.6699418553597415   \n",
       "12/29/2023  1.7746500859264507   3.2305289274978755   0.7956947491701549   \n",
       "\n",
       "                         AMZN.O               NVDA.O                META.O  \\\n",
       "Date                                                                         \n",
       "12/22/2022  0.25180160332983503   0.3516497899131491    0.6373805443775048   \n",
       "12/23/2022   0.2712569778037012   0.4147092641800432    0.7825319323092894   \n",
       "12/27/2022   0.2499884898337587  0.30724380866879264    0.9276709439758826   \n",
       "12/28/2022  0.21139509185122402  0.30418921496222956    0.9320431966710729   \n",
       "12/29/2022  0.20821098456411746   0.5301097048424472    0.8314849065689208   \n",
       "...                         ...                  ...                   ...   \n",
       "12/22/2023   0.6009075771703185   2.5460564786040196   0.01135499596756195   \n",
       "12/26/2023   0.7051192884665655    2.211761126626536                   0.0   \n",
       "12/27/2023   0.7885254062016848    2.914086307569919  0.028695511685780457   \n",
       "12/28/2023   0.6588619348813373   2.9897427640754217    0.3114981992853912   \n",
       "12/29/2023   0.8179580265439256     2.98437678716954    0.5552111303683664   \n",
       "\n",
       "                            BRKb              TSLA.O                  LLY  \\\n",
       "Date                                                                        \n",
       "12/22/2022    0.1807834668597527  0.7724302560148639   0.2783565003979402   \n",
       "12/23/2022   0.05551119178665381  0.9119564700178806   0.5034239236084705   \n",
       "12/27/2022   0.10699529701363969  1.0621971674937245   0.4190859877513477   \n",
       "12/28/2022   0.05827712268428653  1.3914117106322565   0.1499574947947494   \n",
       "12/29/2022  0.006936855074602712   1.685456506642552  0.18366112594290285   \n",
       "...                          ...                 ...                  ...   \n",
       "12/22/2023    1.7413369858893524   9.514757940424353   2.3715672665429492   \n",
       "12/26/2023    1.6993971249898112   9.803992028917644    2.064824856259823   \n",
       "12/27/2023    1.6468981653416397   9.419098544337771   1.7343897836243543   \n",
       "12/28/2023     1.711962643368723   9.212326146741244    1.765189639711142   \n",
       "12/29/2023    1.6294990845944446    9.24328712915325   1.6163103932992242   \n",
       "\n",
       "                              V  ...                MDLZ.O  \\\n",
       "Date                             ...                         \n",
       "12/22/2022  0.08218742305092158  ...   0.22324078878444337   \n",
       "12/23/2022  0.17426407175876968  ...                   0.0   \n",
       "12/27/2022  0.21241606578114655  ...   0.06611834976548546   \n",
       "12/28/2022   0.2201425028732636  ...  0.052474020052158496   \n",
       "12/29/2022  0.37435471264176057  ...    0.1010432041942771   \n",
       "...                         ...  ...                   ...   \n",
       "12/22/2023   0.8543125239697424  ...    1.3360840017366213   \n",
       "12/26/2023   0.9347258229110478  ...    1.3249366751148233   \n",
       "12/27/2023   0.8761851498454007  ...    1.2861829390093644   \n",
       "12/28/2023   0.9244779493256436  ...    1.3729052131441728   \n",
       "12/29/2023    1.146298468163738  ...    1.4298348088631252   \n",
       "\n",
       "                        LRCX.O               REGN.O                  AMT  \\\n",
       "Date                                                                       \n",
       "12/22/2022  0.9112117816397765  0.22467091994047164  0.22911399340818858   \n",
       "12/23/2022  0.9628471566280784   0.2723971667995879  0.45341374943957635   \n",
       "12/27/2022  0.8257298987275898  0.38108087876065994   0.5956294331151347   \n",
       "12/28/2022  0.9520389332371655   0.5258438787249273   0.8783992995040109   \n",
       "12/29/2022  0.9958523928097559   0.7559641431738413    0.917820540512137   \n",
       "...                        ...                  ...                  ...   \n",
       "12/22/2023  2.0481780747922236  0.17480712672920365    3.558225305690294   \n",
       "12/26/2023  2.3013085028654516  0.27790394655287876    3.527400301675957   \n",
       "12/27/2023  2.1792039204752247  0.06671071520238966    3.527529298041054   \n",
       "12/28/2023   2.435340923325042   0.1306652014215108   3.4430788404887824   \n",
       "12/29/2023  2.2988366040369717  0.17288690939135803   3.3143783043087227   \n",
       "\n",
       "                             PGR                ADP.O                  ETN  \\\n",
       "Date                                                                         \n",
       "12/22/2022                   0.0   0.2659701601015139   0.1970186852723305   \n",
       "12/23/2022                   0.0   0.1792559362102821   0.2460887597431093   \n",
       "12/27/2022                   0.0  0.19342556544723746  0.20928144593228293   \n",
       "12/28/2022   0.10586091678166232  0.21318540363602995  0.28942794853369846   \n",
       "12/29/2022  0.014742915153796388  0.10000897944490701  0.30366524279918233   \n",
       "...                          ...                  ...                  ...   \n",
       "12/22/2023    1.0187716000119107   1.4776953860199193   0.5414067073357065   \n",
       "12/26/2023    0.8109193657273481   1.5152058037941434   0.4461575431145655   \n",
       "12/27/2023    0.7891558938096599   1.6041854823635882  0.37339749547366874   \n",
       "12/28/2023    0.6303565888418876    1.407636044733688   0.4156815037033561   \n",
       "12/29/2023   0.48568170417530243   1.3736756078484456  0.44578946033279204   \n",
       "\n",
       "                             MMC                ADI.O                   CB  \n",
       "Date                                                                        \n",
       "12/22/2022   0.24586656277971988   0.4143203164432392  0.05705791978550323  \n",
       "12/23/2022   0.48942905259908154   0.5059598520309163  0.17947667276977897  \n",
       "12/27/2022   0.38814570729822295   0.6734032913299469  0.20377703815077491  \n",
       "12/28/2022    0.4746552733467998   0.4571654651390782  0.15920796494497425  \n",
       "12/29/2022    0.4109609659934432  0.30746992014966235                  0.0  \n",
       "...                          ...                  ...                  ...  \n",
       "12/22/2023   0.16056634179955562  0.11967175932367335    2.566590262648768  \n",
       "12/26/2023   0.08390991022396377  0.13286005617129087   2.6607050384773108  \n",
       "12/27/2023  0.027719767670564367  0.08834600922862032   2.7355854044809305  \n",
       "12/28/2023   0.02995670039482805   0.1270129750934561   2.5085519250989203  \n",
       "12/29/2023   0.11381677036829892  0.21590034828270727   2.5709913257976362  \n",
       "\n",
       "[256 rows x 97 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "367d0ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35969786, 0.40721218, 0.22414987, ..., 0.24586656, 0.41432032,\n",
       "        0.05705792],\n",
       "       [0.50470771, 0.25551051, 0.10239299, ..., 0.48942905, 0.50595985,\n",
       "        0.17947667],\n",
       "       [0.53308684, 0.17581424, 0.1076904 , ..., 0.38814571, 0.67340329,\n",
       "        0.20377704],\n",
       "       ...,\n",
       "       [1.74495784, 3.09721823, 0.67536831, ..., 0.02771977, 0.08834601,\n",
       "        2.7355854 ],\n",
       "       [1.71838971, 3.36507997, 0.66994186, ..., 0.0299567 , 0.12701298,\n",
       "        2.50855193],\n",
       "       [1.77465009, 3.23052893, 0.79569475, ..., 0.11381677, 0.21590035,\n",
       "        2.57099133]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7063f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_RW = mean_squared_error(np.squeeze(Y_test, axis=1), rw_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ec62155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike(y_test, y_pred):\n",
    "    return np.mean(np.log(y_pred+1e-16) + y_test/y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "64246044",
   "metadata": {},
   "outputs": [],
   "source": [
    "QLIKE_LSTM = qlike(Y_test, LSTM_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7b381644",
   "metadata": {},
   "outputs": [],
   "source": [
    "QLIKE_MLP = qlike(Y_test, MLP_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1258809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QLIKE_RW = qlike(Y_test, rw_pred.to_numpy(dtype='float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "55de6193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.06008911e-01, -7.89361330e-02, -1.15252084e-02, ...,\n",
       "         -6.05390604e-01, -1.64439986e-01, -1.90653538e-01],\n",
       "        [-3.06124161e-02, -5.84625659e-02,  9.69521724e-01, ...,\n",
       "         -3.13850545e-01, -9.44265131e-02, -8.67918217e-01],\n",
       "        [-1.06790084e-02,  1.59722506e-01,  8.60168554e-01, ...,\n",
       "         -4.41159048e-01,  4.55334834e-02, -8.42274495e-01],\n",
       "        ...,\n",
       "        [ 7.45649916e-01,  1.23824756e+00,  1.00003651e-01, ...,\n",
       "          3.48866335e+00,  9.34533500e-01,  1.06209872e+00],\n",
       "        [ 7.33228055e-01,  1.31261852e+00,  9.59255936e-02, ...,\n",
       "          3.03801906e+00,  2.74353301e-01,  9.80504808e-01],\n",
       "        [ 7.59361918e-01,  1.27594295e+00,  1.89484220e-01, ...,\n",
       "         -4.50245319e-01, -1.57612206e-01,  1.00361412e+00]],\n",
       "\n",
       "       [[-1.13278103e-01, -8.48446172e-02, -4.05548891e-03, ...,\n",
       "         -6.05350792e-01, -1.66082378e-01, -1.41878480e-01],\n",
       "        [-3.57930641e-02, -6.78790344e-02,  9.85873788e-01, ...,\n",
       "         -3.13830546e-01, -9.57714351e-02, -8.52412005e-01],\n",
       "        [-1.55838622e-02,  1.46037569e-01,  8.75716241e-01, ...,\n",
       "         -4.41133830e-01,  4.45229797e-02, -8.28617395e-01],\n",
       "        ...,\n",
       "        [ 7.44151477e-01,  1.23747073e+00,  1.02482797e-01, ...,\n",
       "          3.48901647e+00,  9.26831097e-01,  1.06311605e+00],\n",
       "        [ 7.31706449e-01,  1.31190353e+00,  9.84248204e-02, ...,\n",
       "          3.03834581e+00,  2.68995766e-01,  9.81614214e-01],\n",
       "        [ 7.57888549e-01,  1.27519818e+00,  1.91588465e-01, ...,\n",
       "         -4.50159318e-01, -1.60764015e-01,  1.00469659e+00]],\n",
       "\n",
       "       [[-1.19866351e-01, -8.35229699e-02,  1.47377658e-02, ...,\n",
       "         -6.10840057e-01, -1.68598280e-01, -1.79353791e-01],\n",
       "        [-4.04884127e-02, -6.57726989e-02,  1.02701436e+00, ...,\n",
       "         -3.16588099e-01, -9.78316562e-02, -8.64325884e-01],\n",
       "        [-2.00292516e-02,  1.49098703e-01,  9.14833058e-01, ...,\n",
       "         -4.44610943e-01,  4.29750378e-02, -8.39110547e-01],\n",
       "        ...,\n",
       "        [ 7.42793405e-01,  1.23764449e+00,  1.08720143e-01, ...,\n",
       "          3.44032823e+00,  9.15032159e-01,  1.06233440e+00],\n",
       "        [ 7.30327379e-01,  1.31206346e+00,  1.04712688e-01, ...,\n",
       "          2.99329323e+00,  2.60788816e-01,  9.80761825e-01],\n",
       "        [ 7.56553200e-01,  1.27536477e+00,  1.96882587e-01, ...,\n",
       "         -4.62017207e-01, -1.65592118e-01,  1.00386490e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.32095270e-01, -5.26309654e-01, -3.23365403e-01, ...,\n",
       "         -3.97695508e-01, -2.91064567e-01, -1.43667947e-01],\n",
       "        [-4.05546534e-01, -7.71450626e-01,  2.86868131e-01, ...,\n",
       "         -2.09514117e-01, -1.98116829e-01, -8.52980900e-01],\n",
       "        [-3.65653329e-01, -8.76461759e-01,  2.11095470e-01, ...,\n",
       "         -3.09596910e-01, -3.23739718e-02, -8.29118449e-01],\n",
       "        ...,\n",
       "        [ 6.37204810e-01,  1.17942834e+00, -3.49386223e-03, ...,\n",
       "          5.33086058e+00,  3.40696413e-01,  1.06307873e+00],\n",
       "        [ 6.23106272e-01,  1.25848134e+00, -8.41023734e-03, ...,\n",
       "          4.74265537e+00, -1.38700064e-01,  9.81573512e-01],\n",
       "        [ 6.52731242e-01,  1.21955096e+00,  1.01637793e-01, ...,\n",
       "         -1.58314684e-03, -4.00609206e-01,  1.00465687e+00]],\n",
       "\n",
       "       [[-6.32563493e-01, -5.38384291e-01, -3.25086633e-01, ...,\n",
       "         -4.12401224e-01, -2.94253361e-01, -2.37384063e-01],\n",
       "        [-4.05880230e-01, -7.90694214e-01,  2.83100163e-01, ...,\n",
       "         -2.16901590e-01, -2.00728067e-01, -8.82774446e-01],\n",
       "        [-3.65969261e-01, -9.04428431e-01,  2.07512853e-01, ...,\n",
       "         -3.18912081e-01, -3.43359192e-02, -8.55359122e-01],\n",
       "        ...,\n",
       "        [ 6.37108293e-01,  1.17784081e+00, -4.06512606e-03, ...,\n",
       "          5.20042500e+00,  3.25741784e-01,  1.06112403e+00],\n",
       "        [ 6.23008263e-01,  1.25702017e+00, -8.98612833e-03, ...,\n",
       "          4.62195970e+00, -1.49102007e-01,  9.79441905e-01],\n",
       "        [ 6.52636339e-01,  1.21802894e+00,  1.01152917e-01, ...,\n",
       "         -3.33503811e-02, -4.06728611e-01,  1.00257703e+00]],\n",
       "\n",
       "       [[-6.32540924e-01, -5.47200674e-01, -3.55686453e-01, ...,\n",
       "         -4.10081158e-01, -2.87357377e-01, -2.02294908e-01],\n",
       "        [-4.05864145e-01, -8.04745059e-01,  2.16113683e-01, ...,\n",
       "         -2.15736095e-01, -1.95081086e-01, -8.71619157e-01],\n",
       "        [-3.65954032e-01, -9.24848498e-01,  1.43821514e-01, ...,\n",
       "         -3.17442461e-01, -3.00930739e-02, -8.45534099e-01],\n",
       "        ...,\n",
       "        [ 6.37112945e-01,  1.17668166e+00, -1.42209857e-02, ...,\n",
       "          5.22100334e+00,  3.58082192e-01,  1.06185590e+00],\n",
       "        [ 6.23012987e-01,  1.25595329e+00, -1.92242493e-02, ...,\n",
       "          4.64100142e+00, -1.26607093e-01,  9.80240021e-01],\n",
       "        [ 6.52640914e-01,  1.21691762e+00,  9.25328455e-02, ...,\n",
       "         -2.83385813e-02, -3.93494978e-01,  1.00335577e+00]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(rw_pred.to_numpy(dtype='float')+1e-16) + Y_test/rw_pred.to_numpy(dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1ec6a14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0078836195"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5322e3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009487135"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "88f97271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7680518863778827"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "280976c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4291024322426299"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLIKE_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d8c50a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.39044982735227746"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLIKE_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a175ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136296388237057.47"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLIKE_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b19bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6014817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb60549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4ca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53542f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674c6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb33a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbfbc4a1",
   "metadata": {},
   "source": [
    "Train, test, and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e42de564",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_80 = int(len(data.index) * .8)\n",
    "q_90 = int(len(data.index) * .9)\n",
    "\n",
    "train, val, test =  data[:q_80], data[q_80:q_90], data[q_90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3fcce",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641fb94",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6fcf339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b76637c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scaler.transform(train)\n",
    "scaled_val = scaler.transform(val)\n",
    "scaled_test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c1f76c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "length=21\n",
    "batch_size = 1024\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length = length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c64865",
   "metadata": {},
   "source": [
    "Model buliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bb277640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_28 (LSTM)              (None, 100)               79200     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 97)                9797      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88997 (347.64 KB)\n",
      "Trainable params: 88997 (347.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from keras import activations\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(length, scaled_train.shape[1])))\n",
    "model.add(Dense(scaled_train.shape[1]))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0df4b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=1)\n",
    "validation_generator = TimeseriesGenerator(scaled_val, scaled_val, \n",
    "                                           length=length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b929e6c",
   "metadata": {},
   "source": [
    "Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1f0e1d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anato\\AppData\\Local\\Temp\\ipykernel_476\\2432282585.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=100,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 278ms/step - loss: 0.0059 - val_loss: 0.0102\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0058 - val_loss: 0.0101\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 1s 153ms/step - loss: 0.0056 - val_loss: 0.0100\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 1s 164ms/step - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 1s 149ms/step - loss: 0.0054 - val_loss: 0.0096\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.0053 - val_loss: 0.0096\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 1s 164ms/step - loss: 0.0052 - val_loss: 0.0097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e136aadc10>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator, epochs=100,\n",
    "                   validation_data=validation_generator,\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7281e",
   "metadata": {},
   "source": [
    "Recursive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9c74a7c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 791ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "n_features = scaled_train.shape[1]\n",
    "LSTM_pred = []\n",
    "\n",
    "first_eval_batch = scaled_train[-length:]\n",
    "current_batch = first_eval_batch.reshape((1, length, n_features))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    LSTM_pred.append(current_pred)\n",
    "    current_batch = np.append(current_batch[:,1:,:], [[current_pred]], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8e32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "75971703",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_pred = scaler.inverse_transform(LSTM_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3e468e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_pred = pd.DataFrame(data=LSTM_pred, columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "29e2eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9766c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_LSTM = mse(test, LSTM_pred).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
